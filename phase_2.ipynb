{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Phase II ##\n",
    "#### Kerwin Chen ####\n",
    "__Research Question:__ How have song lyrics changed over the decades (1960s-2010s)? <br>\n",
    "I'm hoping to analyze trends and changes in lyrics, and also to create a regression model that can predict which decade a song is from based on its lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two parts to my data collection. I first needed to scrape the names of songs of every year from 1960 to 2019 (data scraping pt 1), spanning a total of six decades (60s, 70s, 80s, 90s, 00s, 10s). The Billboard Hot 100 is a standard record chart in the music industry that tracks the most popular songs (through a point system that is a function of the song's sales), and release a year-end top 100 songs of the year every year. There is a website that's been keeping a record of these year-end hot 100s, which I decided to scrape. After scraping that, I used that as an input as a scraped Genius for lyrics via their API. Genius is an American digital media company that provides lyrics for many English and foreign language songs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Pt. I ###\n",
    "Part one of data collection involves __scraping for the name of the song and its artist.__ On initial glance, the top 100 songs of each year seemed to be stored in an html table within appropriate tags and unique identifying tag attributes, all on a page with an easily generated URL. However, after initially naively scrapping every year from 1960 to 2019, I encountered multiple errors here and there. I decided to investigate each page more closely, and found out that the way the data was stored was grossly inconsistent across the years. I was hoping at a worst case scenario to need to scrape each decade individually, but it seems the differences were not dependent on decade. For example, in 2019 the data was stored within p tags, but 2018's data was each stored as an article element. Moreover, 2015 was stored within a table element, while 2013 didn't even have tags. This essentially forced me to check the html file for every single year by hand, and I kept an excel spreadsheet open to help me track how data is stored for each year. This made data collection part one take much much longer than anticipated. I was happy to find that there was consistency from 2012 to 1960 with data stored in rows of a table element, which meant that I could scrape that whole chunk together. For the exploratory analysis, I did not have time to scrap the years outside of 1960 to 2012, but I will in the future scrap 2013-2019 to complete the 2010s decade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade = []\n",
    "year = []\n",
    "artist = []\n",
    "song = []\n",
    "years = list(range(1960,2013))\n",
    "for i in years:\n",
    "    site = \"http://billboardtop100of.com/\" + str(i) + \"-2/\"\n",
    "    webpage = requests.get(site)\n",
    "    soup = bs4.BeautifulSoup(webpage.text, 'html.parser')\n",
    "    table = soup.findAll('tr')\n",
    "    decade += [int(str(i)[:-1] + \"0\")] * 100\n",
    "    year += [i] * 100\n",
    "    artist += [i.text.split(\"\\n\")[2] for i in table]\n",
    "    song += [i.text.split(\"\\n\")[3] for i in table]\n",
    "    #comment below is my final failed attempt at scraping the entire 1960-2019 chunk before noticing the inconsistency\n",
    "    #artist += [i.text.strip() for i in table if \"id\" in i.attrs and not i.text.strip().isnumeric()]\n",
    "    #song += [i.text.strip() for i in table if i.attrs == {} and not i.text.isnumeric()]\n",
    "    print('Done with', i) #allowed me to track the progress of the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(decade), len(year), len(artist), len(song))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, each year should have 100 songs, giving us 1000 songs per decade with $2012 + 1 - 1960 = 53$ total years, resulting in a total of 5300 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1960-2012\n",
    "top100 = pd.DataFrame({'decade': decade, 'year': year, 'artist':artist, 'song':song})\n",
    "top100.to_csv('top100.csv') #stored data into dataframe and exported to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Pt. II ###\n",
    "The second part of my data collection involves taking the list of songs I previously scrapped, and __finding the lyrics for each song.__ To do this, I had to sign up on Genius and make an account to obtain a unique authorization key to access their API. A data scientist named John W. Miller wrote a python package called lyricsgenius that wraps the Genius API, making it easier to download the song lyrics. I used this to get the lyrics of all the songs that I scraped from 1960-2012. I used a try-except block to handle cases were the song lyrics weren't found. This process ended up taking almost four hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius as lg\n",
    "lyrics = []\n",
    "genius = lg.Genius(\"JAlCQvWQxOy0Ertp8NhDj4wHzxBwc12vQiToA2HkRFqDHCLOBYTj0DjgrZCldg0f\")\n",
    "progress = 500\n",
    "for i in range(len(top100.index)):\n",
    "    locate_song = genius.search_song(top100[\"song\"][i], top100[\"artist\"][i])\n",
    "    try:\n",
    "        lyrics += [locate_song.lyrics]\n",
    "    except AttributeError:\n",
    "        lyrics += [\"NONE\"] #if error thrown (ie there is no lyrics), put \"NONE\" into the lyrics list\n",
    "    if i % progress == 0:\n",
    "        print(\"Done with\", i, \"songs\") #allowed me to track the progress of the scraping\n",
    "        progress += 500\n",
    "print(\"Done with all songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100['lyrics'] = lyrics\n",
    "top100.to_csv('top100Ly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_lyrics = top100.loc[top100[\"lyrics\"] == \"NONE\"].copy().groupby(\"decade\")\n",
    "no_lyrics.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I knew that there were going to be songs where I couldn't find the lyrics to, and I was initially worried that there would be a tendency for older songs to not have lyrics, giving me a smaller and smaller sample of songs as we go back in time. However, I displayed the number of songs with missing lyrics by decade, and every decade seemed to have about 10-20 songs missing. The 2010's so far only has one song missing, but that may also be because it currently only has songs from 2010-2012. I don't think the 10-20 missing song per decade was something I needed to worry about, because they represent a mere 1-2% of the 1000 songs per decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This collections module has a Counter object that would allow me to quickly tally frequency of words within songs into dictionary-like objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_bydec = top100.groupby(by='decade')\n",
    "count_bydec = []\n",
    "#top100_byyear.get_group(1960)[\"lyrics\"]\n",
    "for i in range(1960, 2020, 10)`: #looping through each decade\n",
    "    dec_lyrics = top100_bydec.get_group(i)[\"lyrics\"] #getting lyrics of each group\n",
    "    count_bydec.append(Counter(\"\".join(filter(lambda x: x not in [\",\", \".\", \"!\", \"?\"], dec_lyrics)).lower().split()))\n",
    "    #append to a new list a Counter object containing frequency of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I wanted words like \"Love\", \"love\", and \"love,\" to all count as the same word, I made sure to remove punctuation using the filter function and an anonymous function, and setting everything to lowercase letters, before I passed it into a Counter object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_bydec) #to confirm number of decades we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_bydec)\n",
    "x = 1960\n",
    "for i in count_bydec:\n",
    "    print(\"Decade\", x, \":\\n\", i.most_common(10), \"\\n\")\n",
    "    x += 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, the top 10 words of each decade weren't unique and insightful, as they were just common English words. I plan to remove these to find unique words of each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_counter = pd.DataFrame({'decade' : list(range(1960, 2020, 10)), 'count_bydec': count_bydec})\n",
    "decade_counter.to_csv('count_bydec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"love\",\"baby\",\"money\", \"gonna\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of Select Generic Words in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a note, due to the 2010s having only 300 songs so far, its not very representative of that decade. I selected a few words that I noticed from personal experience showed up in songs frequently. As suspected, love is a common word found in songs of all decades, and the word _money_ having a peak during the 70's and _baby_ peaking in the 90's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"phone\", \"radio\", \"stereo\", \"pager\", \"tape\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of Select Technological Words in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this plot I selected key words related to technologies used in daily life and took a look at their frequency. Something interesting to note is the small peak during the 80's for the word _stereo_, and then a higher peak for the 2010's.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"bitch\",\"fuck\", \"hoes\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "censor = lambda x: x[0] + \"*\" + x[2:]\n",
    "plt.legend([censor(i) for i in keywords])\n",
    "plt.title(\"Frequency of Select Profanities/Derogatory Terms in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at frequency of profanities, we see a huge jump during the 2000s. I suspect that when I finish collecting data for the 2010s, I will see a similarly high peak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"sunshine\", \"moon\", \"river\", \"sun\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of Select Nature Words in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 80s we see a dip in words related to nature, and a general decrease since."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"disco\", \"party\", \"bar\", \"club\", \"dance\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of Entertainment Related Words in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rise of _disco_ in the 70's is reflected in song lyrics of that time as we can see a small peak in our graph. We also see a large peak for _club_ and _party_ as we reach the 2000s. _Dance_ shows a large peak in both the 70's and 2000's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"girl\", \"boy\", \"woman\", \"man\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of Select Gender Words in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this graph that the most common method to refer to females seem to be _girl_, while for males, _man_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"ass\", \"butt\", \"bum\", \"bottom\"]\n",
    "for i in keywords:\n",
    "    plt.plot(list(range(1960,2020,10)), [j[i] for j in count_bydec])\n",
    "plt.legend(keywords)\n",
    "plt.title(\"Frequency of variations of buttocks in Songs Over Time\")\n",
    "plt.xlabel(\"decade\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description ###\n",
    "__What are the observations (rows) and the attributes (columns)?__ <br>\n",
    "The attributes were the decade, and the count of words by decade.\n",
    "Each observation (row) represents a decade, and the count of words is stored as Counter objects.<br>\n",
    "__What processes might have influenced what data was observed and recorded and what was not?__<br>\n",
    "Because the creators of the data (Billboard) isn't involved with the music itself, I feel that it's able to maintain a pretty objective perspective because its ranking is based on a public function of music popularity and cumulative sales. <br>\n",
    "__What preprocessing was done, and how did the data come to be in the form that you are using?__<br>\n",
    "I obtained this dataset by first scraping for the top 100 songs of each year from Billboard Hot 100. Using this, I scraped for the lyrics for each of these songs, and then grouped them by decade before finding the frequency fo each word.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Limitations ###\n",
    "One limitation of my data is its size. Although I have 1000 songs per decade, that's really only 100 songs a year, which I feel like could have an impact on the results, so it would have been better if I could obtain more songs per year. I'm also still missing data for 2013-2019, so my 2010's data is still not representative of the decade. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question for Reviewers ###\n",
    "My data is pretty qualitative, so I'm having trouble with coming up with other analysis ideas other than the ones I have with comparing word frequency over time. I plan to also create a regression model that can predict decade based on lyrics, but other than that, what other analysis would you suggest?\n",
    "\n",
    "How can I clarify my research question and make it better?\n",
    "\n",
    "Any other improvements/suggestions/words to take a look at are welcomed :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
