{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two parts to my data collection. I first needed to scrape the names of songs of every year from 1960 to 2019 (data scraping pt 1), spanning a total of six decades (60s, 70s, 80s, 90s, 00s, 10s). The Billboard Hot 100 is a standard record chart in the music industry that tracks the most popular songs (through a point system that is a function of the song's sales), and release a year-end top 100 songs of the year every year. There is a website that's been keeping a record of these year-end hot 100s, which I decided to scrape. After scraping that, I used that as an input as a scraped Genius for lyrics via their API. Genius is an American digital media company that provides lyrics for many English and foreign language songs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Pt. I ###\n",
    "Part one of data collection involves __scraping for the name of the song and its artist.__ On initial glance, the top 100 songs of each year seemed to be stored in an html table within appropriate tags and unique identifying tag attributes, all on a page with an easily generated URL. However, after initially naively scrapping every year from 1960 to 2019, I encountered multiple errors here and there. I decided to investigate each page more closely, and found out that the way the data was stored was grossly inconsistent across the years. I was hoping at a worst case scenario to need to scrape each decade individually, but it seems the differences were not dependent on decade. For example, in 2019 the data was stored within p tags, but 2018's data was each stored as an article element. Moreover, 2015 was stored within a table element, while 2013 didn't even have tags. This essentially forced me to check the html file for every single year by hand, and I kept an excel spreadsheet open to help me track how data is stored for each year. This made data collection part one take much much longer than anticipated. I was happy to find that there was consistency from 2012 to 1960 with data stored in rows of a table element, which meant that I could scrape that whole chunk together. \n",
    "\n",
    "[Data Collection Pt. I, 1960-2012](url)\n",
    "\n",
    "Scraping the song titles/artists for 2013-2019 took a while. I weighed the pros and cons of scraping each page individually, and I eventually decided that it would be faster if I just manually copy and pasted each page of songs and artists into an excel sheet, and then reading each csv file into Jupyter for processing, and then appending all of the processed songs/artists together into one csv. I attempted to scrape the song lyrics as well, but there were a lot more missing lyrics than there should be for such a recent decade, and decided to drop it for now and focus on the songs and artists.\n",
    "\n",
    "[Data Collection Pt. I, 2013-2019](url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Pt. II ###\n",
    "The second part of my data collection involves taking the list of songs I previously scrapped, and __finding the lyrics for each song.__ To do this, I had to sign up on Genius and make an account to obtain a unique authorization key to access their API. A data scientist named John W. Miller wrote a python package called lyricsgenius that wraps the Genius API, making it easier to download the song lyrics. I used this to get the lyrics of all the songs that I scraped from 1960-2012. I used a try-except block to handle cases where the song lyrics weren't found. This process ended up taking almost four hours to complete, and a bit of time for 2013-2019 as well. \n",
    "\n",
    "[Data Collection Pt. II, 1960-2012](url)<br>\n",
    "[Data Collection Pt. II, 2013-2019](url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning ###\n",
    "I also scraped the lyrics for 2013-2019 and attempted to subsequently start the data analysis before realizing two issues. The first, more minor, is the fact that the lyrics weren't very clean (eg \"\\[Verse 1\\]\" found in the songs, punctuation, etc). The second issue I found out was the fact that even though I was able to successfully web scrape from Genius using the song and artists, I didn't always get back the right song - sometimes I would get a random whole book or movie script, or the lyrics would be from the wrong song, or the lyrics would be in a different alnguage (even if the song was in English). I initially started by manually recording and removing the indexes of songs with issues, and after manually identifying ~50 indexes and noticing many more to go, I decided to throw away this whole thing and did some experimenting with scraping from Genius. I noticed a few key things that increased the rate of successful scrapings:\n",
    "1) if the song has a / in the name, removing the / and everything after it<br>\n",
    "2) if the song has featured artists, removing feat. and everything after it<br>\n",
    "\n",
    "After scraping all the song lyrics again, which took another few hours, I tried a few macroscopic filters to remove incorrect scrapings:\n",
    "1) removing lyrics with just \\[instrumental\\] $\\longrightarrow$ successful in removing incorrect song lyrics<br>\n",
    "2) removing lyrics without \\\\n $\\longrightarrow$ successful in removing incorrect song lyrics<br>\n",
    "3) removing lyrics without \"\\[\" (from \\[VERSE\\], etc) $\\longrightarrow$ unsuccessful, because many of the older songs tended not to have these tags that differentiated the different portions of a song<br>\n",
    "3) removing lyrics with no date given from Genius (scraped the song publication date by Genius as well) $\\longrightarrow$ unsuccessful, because many songs turned out to just not have dates provided by Genius<br>\n",
    "4) for songs WITH dates given from Genius, removing lyrics for songs that rose to fame after >3 years $\\longrightarrow$ unsuccessful, because it turns out that quite a few songs rose to fame after >3 years (>5 even), and upon inspection the lyrics were correct<br>\n",
    "5) removing lyrics that have more than 6000 characters (this number was determined upon experimenting)$\\longrightarrow$ successful in removing incorrect song lyrics, though a few viable lyrics may have been removed as well, but I figured it was a small enough number to not matter\n",
    "\n",
    "After removing as much as possible the incorrect lyrics, I continued by removing tags in the songs such as \\[VERSE\\] or \\[CHORUS\\], removing punctuation, and removing newlines, before finally saving the cleaned dataset as my final csv.\n",
    "\n",
    "[Data Cleaning](url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
